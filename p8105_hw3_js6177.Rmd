---
title: "p8105_hw3_js6177"
author: "Jiayi Shi"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

Sys.setlocale("LC_TIME", "English")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

```{r}
library(p8105.datasets)
data("instacart")
```

## Problem 2

### tidy data

The code below reads the data, cleans the variables' names, create a variable `day_type` that specifies weekday or weekend of the associated day, convert `day` into a factor variable and arrange the columns to produce the final dataset.

```{r}
accel = read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    day_type = ifelse(day %in% c("Sunday","Saturday"),"weekend","weekday"),
    day = as.factor(day)
  ) %>% 
  select(week,day_id,day,day_type,everything())
```

The resulting dataset has `r nrow(accel)` observations and `r ncol(accel)` variables, including `week` (week number), `day_id` (day identifier), a factor variable `day` (Mon-Sun), a character variable `day_type` (weekday vs weekend) and `activity_*` (activity counts for each minute of a 24-hour day). 

### total activity

The code belows create a variable `total_activity` by summing up the activity counts for each minute of the associated day.

```{r}
accel %>% 
  mutate(
    total_activity = select(.,starts_with("activity_")) %>% rowSums(na.rm = T)
         ) %>% 
  select(day_id,total_activity) %>% 
  knitr::kable(col.names = c("Day id","Total activity counts"))
```

From the table, I cannot see any apparent trend. 

### Plot

The codes below convert the original dataset from wide to long, and create a variable `hr_activity` which is the summation of the activity counts for each minute within that hour. I then group dataset by `day` and `activity_hr` and make a plot of 24-hour activity time courses of each day of the week.

```{r}
accel %>%  
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_min",
    names_prefix = "activity_",
    values_to = "min_count"
  ) %>%  
  mutate(
    activity_min = as.integer(activity_min),
    activity_hr = activity_min %/% 60
  ) %>% 
  group_by(day,activity_hr) %>% 
  mutate(hr_count = sum(min_count)) %>% 
  
  ggplot(aes(x = activity_hr, y = hr_count, color = day)) +
  geom_point()+
  geom_line()+
  labs(
    title = "Plot of 24-hour activity time courses",
    x = "Activity time (hour)",
    y = "Activity counts"
  )+
  scale_color_discrete(name = "Day of the week")
```

Most activity happened between 6:00-22:00. On Monday and Friday night (20:00-21:00) as well as Saturday morning (9:00-11:00) we see spikes in activity counts. Activity is very low on Sunday. I then group dataset by `day_type` and the graph below clearly shows that the mean activity count of weekend is lower than that of weekday.  

```{r}
accel %>%  
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_min",
    names_prefix = "activity_",
    values_to = "min_count"
  ) %>%  
  mutate(
    activity_min = as.integer(activity_min),
    activity_hr = activity_min %/% 60
  ) %>% 
  group_by(day_type,activity_hr) %>% 
  mutate(hr_count = sum(min_count),
         mean_hr_count = mean(hr_count)
         ) %>% 
  
  ggplot(aes(x = activity_hr, y = mean_hr_count, color = day_type)) +
  geom_point()+
  geom_line()+
  labs(
    title = "Plot of 24-hour activity time courses",
    x = "Activity time (hour)",
    y = "Mean activity counts"
  )
```



## Problem 3

### Description

```{r}
library(p8105.datasets)
data("ny_noaa")
```

The size of the dataset is `r dim(ny_noaa)[1]` x `r dim(ny_noaa)[2]`, with`r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables, including `id` (Weather station ID), `date` (Date of observation), `prcp` (Precipitation (tenths of mm)), `snow` (Snowfall (mm)), `snwd` (Snow depth (mm)), `tmax` (Maximum temperature (tenths of degrees C)) and `tmin` (Minimum temperature (tenths of degrees C)). 

The codes below shows the structure of the dataset.

```{r}
str(ny_noaa)
```

`id`, `tmax`, `tmin` are character variables, `date` is date variable, and `prcp`, `snow`, `snwd` are integer variables.

The proportion of missing values `r round(sum(is.na(ny_noaa$prcp))/nrow(ny_noaa),2)` for `prcp`, `r round(sum(is.na(ny_noaa$snow))/nrow(ny_noaa),2)` for `snow`, `r round(sum(is.na(ny_noaa$snwd))/nrow(ny_noaa),2)` for `snwd`, `r round(sum(is.na(ny_noaa$tmax))/nrow(ny_noaa),2)` for `tmax`, `r round(sum(is.na(ny_noaa$tmin))/nrow(ny_noaa),2)` for `tmin`. Since most of them is larger than 10%, I think the missing value will influence the exploratory numeric analysis to some extent.

### (1)

The code belows do the data cleaning: separate `date` variable into `year`, `month`, and `day`, convert`tmin` and `tmax` from character to numeric variables, and ensure observations for temperature, precipitation, and snowfall are given in reasonable units.

```{r}
ny_noaa_tidy = 
  mutate(ny_noaa,
    year = as.integer(lubridate::year(date)), 
    month = as.integer(lubridate::month(date)), 
    day = as.integer(lubridate::day(date)),
    prcp = prcp/10,
    tmin = as.numeric(tmin)/10,
    tmax = as.numeric(tmax)/10
    ) %>% 
  select(id,date,year,month,day,prcp:tmin) 
  
ny_noaa_tidy %>% summarise(median_snow = median(snow, na.rm = T))
```

The most commonly observed values for snowfall is 0 mm (using R inline code to calculate median). 

### (2)

I use `group_by` based on `id`, `year` and `month` to calculate the average max temperature in January and July in each station respectively, and make a two-panel scatter plot to show the average `tmax` across years.

```{r}
ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month %in% c("Jan","Jul")) %>% 
  group_by(id,year,month) %>% 
  mutate(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  filter(!(is.na(avg_tmax))) %>% 
  
  ggplot(aes(x = year, y = avg_tmax, color = id))+
  geom_point(alpha = .1) +
  facet_grid(month ~., scales = "free_y") +
  labs(
    title = "Average maximum temperature in Jan and Jul in each station",
    x = "year",
    y = "Average maximum daily temperature (C)",
    caption = "Data from NOAA")+
  theme(legend.position = "none")
```  

From the plot, it seems that both the distributions are normal. The average max temperature in January centers around -0.2C while that in July centers around 27C. The max temperature of January varies more than that of July.

I use boxplot to identify outliers.

```{r}
ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month %in% c("Jan","Jul")) %>% 
  group_by(id,month) %>% 
  mutate(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  filter(!(is.na(avg_tmax))) %>% 
  ggplot(aes(x = month, y = avg_tmax))+
  geom_boxplot()+
  labs(y = "Average maximum daily temperature (C)",
    x = "Month",
    caption = "Data from NOAA")
```

Therefore, I can find the outliers for average max temperature in January and July respectively using the following code.

```{r}
rbind(
  ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month == "Jan") %>% 
  group_by(id) %>% 
  summarise(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  filter(min_rank(avg_tmax)<4),
  ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month == "Jan") %>% 
  group_by(id) %>% 
  summarise(avg_tmax = mean(tmax, na.rm = TRUE)) %>%
  filter(min_rank(desc(avg_tmax))<3))
```

```{r}
rbind(
  ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month == "Jul") %>% 
  group_by(id) %>% 
  summarise(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  filter(min_rank(avg_tmax)<3),
  ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month == "Jul") %>% 
  group_by(id) %>% 
  summarise(avg_tmax = mean(tmax, na.rm = TRUE)) %>%
  filter(min_rank(desc(avg_tmax))<3))
```

### (3)

To better reduce overlapping, I use hexagonal heatmap instead of traditional scatterplot to plot `tmax` vs `tmin`. I use ridgeline visualization to show the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r warning=FALSE}
tmax_tmin = 
  ny_noaa_tidy %>% 
  ggplot(aes(x = tmin, y = tmax))+
  geom_hex()+
    labs(
    title = "Maximum temperature vs minimum temperature",
    x = "Minimum temperature (C)",
    y = "Maximum temperature (C)"
  ) +
  theme(legend.position = "right",
        plot.title = element_text(size = 13))

snowfall = 
  ny_noaa_tidy %>%
  filter(snow>0,snow<100) %>%
  mutate(year = as.character(year)) %>% 
  ggplot(aes(x = snow, y = year)) + 
  geom_density_ridges(scale = 0.85)+
  labs(
    title = "The distribution of snowfall in each year",
    y = "Year",
    x = "Snow fall (mm)",
    caption = "Data from NOAA"
  ) +
  scale_y_discrete(breaks = c("1981", "1986", "1991", "1996", "2001", "2006", "2010"))+
  theme(plot.title = element_text(size = 13))

tmax_tmin / snowfall + plot_layout(ncol = 1, heights = c(1.5,2))
```

From the plot `tmax_tmin`, we can see that there are some outliers, where extreme temperatures are reached. This may be because of wrong records based on common sense. The maximum temperature centers around 14C while the minimum temperature centers around 3C.

The distributions of snowfall for all years are right-skewed, centering around 20-30mm. The maximum snowfall can reach over 80, which indicates either a horrible disaster or wrong recording. 




