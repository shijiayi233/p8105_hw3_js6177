---
title: "p8105_hw3_js6177"
author: "Jiayi Shi"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

Sys.setlocale("LC_TIME", "English")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

```{r}
library(p8105.datasets)
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

his dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns, with each row resprenting a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are several order-level variables, describing the day and time of the order, and number of days since prior order. Then there are several item-specific variables, describing the product name (e.g. Yogurt, Avocado), department (e.g. dairy and eggs, produce), and aisle (e.g. yogurt, fresh fruits), and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles, with fresh vegetables and fresh fruits holding the most items ordered by far.

```{r}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.

```{r}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered in your table.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

Finally is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  #spread(key = order_dow, value = mean_hour) %>%
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean_hour"
  ) %>% 
  knitr::kable(digits = 2)
```

## Problem 2

### tidy data

The code below reads the data, cleans the variables' names, create a variable `day_type` that specifies weekday or weekend of the associated day, convert `day_type` and `day` into factor variables. I then pivot the dataset form wide to long, transforming all of those 1,440 columns into just two columns: `activity_num` and `activity_count`. I then arrange the columns to produce the final dataset.

```{r}
accel = read_csv("data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    day_type = ifelse(day %in% c("Sunday","Saturday"),"weekend","weekday"),
    day_type = as.factor(day_type),
    day = as.factor(day)
  ) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_num",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  select(week,day_id,day,day_type,everything())
```

The resulting dataset has `r nrow(accel)` observations and `r ncol(accel)` variables. Each row shows activity counts for each minute of a specific day. Variables including `week` (week number), `day_id` (day identifier), a factor variable `day` (Mon-Sun), a factor variable `day_type` (weekday vs weekend), `activity_num` which is the minute of the day ranging from 1 to 1440, `activity_count` the activity count of that minute on the associated day.

### total activity

The code belows create a variable `total_activity` by summing up the activity counts for each minute of the associated day.

```{r}
accel %>% 
  group_by(day_id) %>% 
  summarise(total_activity = sum(activity_count)) %>%
  knitr::kable(col.names = c("Day id","Total activity counts"),digit = 1)
```

From the table, I cannot see any apparent trend. 

The table below shows the number of day and the total activity counts for the associated day, which is orded by the total activity counts.
```{r}
accel %>% 
  group_by(day) %>% 
  summarise(day_count = n()/1440,
            total_activity = sum(activity_count)) %>% 
  arrange(desc(total_activity)) %>% 
  knitr::kable()
```
From this table, I can see that the activity counts fluctuate throughout the week.

### Plot

The codes below convert the original dataset from wide to long, and create a variable `hr_activity` which is the summation of the activity counts for each minute within that hour. I then group dataset by `day` and `activity_hr` and make a plot of 24-hour activity time courses of each day of the week.

```{r}
accel %>%  
  ggplot(aes(x = activity_num, y = activity_count, color = day)) +
  geom_point()+
  geom_line()+
  labs(
    title = "Plot of 24-hour activity time courses",
    x = "Time (minute)",
    y = "Activity counts"
  )+
  scale_color_discrete(name = "Day of the week")
```

Since the plot is too messy, I convert the unit of x-axis from minute to hour.

```{r}
accel %>%  
  mutate(
    activity_num = as.integer(activity_num),
    activity_hr = activity_num %/% 60
  ) %>% 
  group_by(day,activity_hr) %>% 
  mutate(hr_count = sum(activity_count)) %>% 
  
  ggplot(aes(x = activity_hr, y = hr_count, color = day)) +
  geom_point()+
  geom_line()+
  labs(
    title = "Plot of 24-hour activity time courses",
    x = "Time (hour)",
    y = "Activity counts"
  )+
  scale_color_discrete(name = "Day of the week")
```

From the plot, we can see that most activity happened between 6:00-22:00. On Monday and Friday night (20:00-21:00) as well as Saturday morning (9:00-11:00) we see spikes in activity counts. Activity is very low on Sunday. I then group dataset by `day_type` and the graph below clearly shows that the mean activity count of weekend is lower than that of weekday.  

```{r}
accel %>%  
  mutate(
    activity_num = as.integer(activity_num),
    activity_hr = activity_num %/% 60
  ) %>% 
  group_by(day_type,activity_hr) %>% 
  mutate(hr_count = sum(activity_count),
         mean_hr_count = mean(hr_count)
         ) %>% 
  
  ggplot(aes(x = activity_hr, y = mean_hr_count, color = day_type)) +
  geom_point()+
  geom_line()+
  labs(
    title = "Plot of 24-hour activity time courses",
    x = "Time (hour)",
    y = "Mean activity counts"
  )+
  viridis::scale_color_viridis(
    name = "Day type",
    discrete = TRUE) 
```


## Problem 3

### Description

```{r}
library(p8105.datasets)
data("ny_noaa")
```

The size of the dataset is `r dim(ny_noaa)[1]` x `r dim(ny_noaa)[2]`, with`r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables, including `id` (Weather station ID), `date` (Date of observation), `prcp` (Precipitation (tenths of mm)), `snow` (Snowfall (mm)), `snwd` (Snow depth (mm)), `tmax` (Maximum temperature (tenths of degrees C)) and `tmin` (Minimum temperature (tenths of degrees C)). 

The codes below shows the structure of the dataset.

```{r}
str(ny_noaa)
```

`id`, `tmax`, `tmin` are character variables, `date` is date variable, and `prcp`, `snow`, `snwd` are integer variables.

The proportion of missing values `r round(sum(is.na(ny_noaa$prcp))/nrow(ny_noaa),2)` for `prcp`, `r round(sum(is.na(ny_noaa$snow))/nrow(ny_noaa),2)` for `snow`, `r round(sum(is.na(ny_noaa$snwd))/nrow(ny_noaa),2)` for `snwd`, `r round(sum(is.na(ny_noaa$tmax))/nrow(ny_noaa),2)` for `tmax`, `r round(sum(is.na(ny_noaa$tmin))/nrow(ny_noaa),2)` for `tmin`. Since most of them is larger than 10%, I think the missing value will influence the exploratory numeric analysis to some extent.

### (1)

The code belows do the data cleaning: separate `date` variable into `year`, `month`, and `day`, convert`tmin` and `tmax` from character to numeric variables, and ensure observations for temperature, precipitation, and snowfall are given in reasonable units.

```{r}
ny_noaa_tidy = 
  mutate(ny_noaa,
    year = as.integer(lubridate::year(date)), 
    month = as.integer(lubridate::month(date)), 
    day = as.integer(lubridate::day(date)),
    prcp = prcp/10,
    tmin = as.numeric(tmin)/10,
    tmax = as.numeric(tmax)/10
    ) %>% 
  select(id,date,year,month,day,prcp:tmin) 
  
ny_noaa_tidy %>% summarise(median_snow = median(snow, na.rm = T))
```

The most commonly observed values for snowfall is 0 mm (using R inline code to calculate median). 

### (2)

I use `group_by` based on `id`, `year` and `month` to calculate the average max temperature in January and July in each station respectively, and make a two-panel Spaghetti plot to show the average `tmax` across years.

```{r}
ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month %in% c("Jan","Jul")) %>% 
  group_by(id,year,month) %>% 
  mutate(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  filter(!(is.na(avg_tmax))) %>% 
  
  ggplot(aes(x = year, y = avg_tmax, color = id))+
  geom_line() +
  facet_grid(month ~., scales = "free_y") +
  labs(
    title = "Average maximum temperature in Jan and Jul in each station",
    x = "year",
    y = "Average maximum daily temperature (C)",
    caption = "Data from NOAA")+
  scale_x_continuous(breaks = seq(1981,2010,1))+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+
  theme(legend.position = "none")
```  

From the plot, it seems that both the distributions are normal. The average max temperature in January centers around -0.2C while that in July centers around 27C. The max temperature of January varies more than that of July.

I use boxplot to identify outliers.

```{r}
ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month %in% c("Jan","Jul")) %>% 
  group_by(id,month) %>% 
  mutate(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  filter(!(is.na(avg_tmax))) %>% 
  ggplot(aes(x = month, y = avg_tmax))+
  geom_boxplot()+
  labs(y = "Average maximum daily temperature (C)",
    x = "Month",
    caption = "Data from NOAA")
```

Therefore, I can find the outliers for average max temperature in January and July respectively using the following code.

```{r}
rbind(
  ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month == "Jan") %>% 
  group_by(id) %>% 
  summarise(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  filter(min_rank(avg_tmax)<4),
  ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month == "Jan") %>% 
  group_by(id) %>% 
  summarise(avg_tmax = mean(tmax, na.rm = TRUE)) %>%
  filter(min_rank(desc(avg_tmax))<3))
```

```{r}
rbind(
  ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month == "Jul") %>% 
  group_by(id) %>% 
  summarise(avg_tmax = mean(tmax, na.rm = TRUE)) %>% 
  filter(min_rank(avg_tmax)<3),
  ny_noaa_tidy %>% 
  mutate(month = month.abb[month]) %>% 
  filter(month == "Jul") %>% 
  group_by(id) %>% 
  summarise(avg_tmax = mean(tmax, na.rm = TRUE)) %>%
  filter(min_rank(desc(avg_tmax))<3))
```

### (3)

To better reduce overlapping, I use hexagonal heatmap instead of traditional scatterplot to plot `tmax` vs `tmin`. I use boxplot to show the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r warning=FALSE}
tmax_tmin = 
  ny_noaa_tidy %>% 
  ggplot(aes(x = tmin, y = tmax))+
  geom_hex()+
    labs(
    title = "Max temperature vs min temperature",
    x = "Minimum temperature (C)",
    y = "Maximum temperature (C)"
  ) +
  theme(plot.title = element_text(size = 11),
        plot.title.position = "plot",
        legend.key.width = unit(1,"cm"))

snowfall = 
  ny_noaa_tidy %>%
  filter(snow>0,snow<100) %>%
  mutate(year = as.character(year)) %>% 
  ggplot(aes(x = snow, y = year)) + 
  geom_boxplot()+
  labs(
    title = "Distribution of snowfall in each year",
    y = "Year",
    x = "Snow fall (mm)",
    caption = "Data from NOAA"
  ) +
  #scale_y_discrete(breaks = c("1981", "1986", "1991", "1996", "2001", "2006", "2010"))+
  theme(plot.title = element_text(size = 11),
        plot.title.position = "plot",
        plot.caption.position = "panel")

#tmax_tmin / snowfall + plot_layout(ncol = 1, heights = c(1.5,2))
tmax_tmin + snowfall
```

From the plot `tmax_tmin`, we can see that there are some outliers, where extreme temperatures are reached. This may be because of wrong records based on common sense. The maximum temperature centers around 14C while the minimum temperature centers around 3C.

The distributions of snowfall for all years are right-skewed, centering around 20-30mm and median remains around 25mm.




